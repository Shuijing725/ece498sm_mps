{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install cmake 'gym[atari]' scipy numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules imported!\n"
     ]
    }
   ],
   "source": [
    "#Understand the **Taxi-v2** environment in gym. We first create the environment\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print (\"modules imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|R: | : :G|\n| : : : : |\n| : : : : |\n| | : | : |\n|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n+---------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Create the taxi-v2 environment\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "\n",
    "env.render() #creates the simulation in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|R: | : :\u001b[34;1mG\u001b[0m|\n| : : : : |\n| : : : : |\n| | : | : |\n|\u001b[35mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n+---------+\n\nAction Space:  Discrete(6)\nState Space:  Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space: \", env.action_space) #action space of the Taxi-environment\n",
    "print(\"State Space: \", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first understand the state-space of this problem. We define the state of the taxi with the following co-ordinates \n",
    "\n",
    "(row, column, passenger_index, destination_index)\n",
    "\n",
    "where, *row* and *column* denotes the location of the taxi in a 5X5 environment. \n",
    "*passenger_index* : is the index number of passenger in the taxi. It could either be empty(0) or filled with either one of the passengers(1,2,3,4). Similarly, destination index is one of the four locations of the taxi(1,2,3,4) for one of the four locations. Thus, the total possible state space of the problem is as follows, 5X5X5X4 = 500\n",
    "\n",
    "The action space of this algorithm has six values. Of these the first four values denote the direction of car's going. The next two action space is to either pick-up or drop-off the car at the particular location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|R: | : :\u001b[34;1mG\u001b[0m|\n| : : : : |\n| : : : : |\n| | : | : |\n|\u001b[35mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n+---------+\n\nencoded state 328\ndecoded_state [3 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "state_e = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "state_d = env.decode(state_e)\n",
    "env.render()\n",
    "print (\"encoded state\", state_e)\n",
    "print (\"decoded_state\", np.array(list(state_d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 552\nPenalties incurred: 175\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Let's now, run a policy wherein the actions are randomly choosen\n",
    "to get an idea how good or worse a random policy performs and the \n",
    "'''\n",
    "env.reset()  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken:\", format(epochs))\n",
    "print(\"Penalties incurred:\",format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning Algorithm\n",
    "In the next cell. You are required to implement the Q-learning algorithm. \n",
    "\n",
    "This has been described in MP4 write up as well as in class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q-learning algorithm implementation\n",
    "# Set Hyper-parameters for the Q-learning algorithm\n",
    "alpha = 0.1 # learning rate\n",
    "gamma = 0.6\n",
    "epsilon = 0.1 #epsilon value for epsilon-greedy policy\n",
    "# Initialise the Q-table for the problem\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "# Survelliance data for the whole run of this algorithm\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "M = 100001 # episodic runs of the Q-learning algorithm\n",
    "for i in range(1, M):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        ###\n",
    "        #Add you code here. \n",
    "        #Remember action in line 24 would be based on the epsilon-greedy policy\n",
    "        #described above\n",
    "        ###\n",
    "        random_num = np.random.uniform(0, 1)\n",
    "        if random_num < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, info = env.step(action) #next state\n",
    "        #env.render() #render the simulation to see it on the screen\n",
    "        \n",
    "\n",
    "        ###\n",
    "        # Add the remaining part of the code here\n",
    "        # Update Q table after taking action above\n",
    "        ###\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        q_table[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        \n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        if epochs > 100:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Episode:\", i)\n",
    "\n",
    "print(\"Training finished.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Test your Q-learning implementation in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after {episodes} episodes:\n",
      "Average timesteps per episode: 12.63\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Testing Q-learning implementation\n",
    "def test(q_table, episodes = 100):\n",
    "    total_epochs, total_penalties = 0, 0\n",
    "    #episodes = 100\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        epochs, penalties, reward = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state])\n",
    "            state, reward, done, info = env.step(action)\n",
    "            #print (state)\n",
    "            #env.render()\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "\n",
    "            epochs += 1\n",
    "            \n",
    "            if epochs > 100:\n",
    "                break\n",
    "\n",
    "        total_penalties += penalties\n",
    "        total_epochs += epochs\n",
    "\n",
    "    print(\"Results after {episodes} episodes:\")\n",
    "    print(\"Average timesteps per episode:\", total_epochs/episodes)\n",
    "    print(\"Average penalties per episode:\", total_penalties/episodes)\n",
    "test(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Function Approximations in Reinforcement learning\n",
    "You will observe that training a simple environment with state, action pairs of 500×6 elements takes a long time to converge using Q-learning. Additionally, you have to create 500×6 element size Q-table to store all the Q-values. This process of making such a large Q-table is memory intensive. You also may have noticed that this method of using a Q-table only works when the state and action spaces is discrete. These practical issues with Q-learning was what originally prevented it being from used in many real-world scenarios.\n",
    "\n",
    "If the state space/action spaces are large, an alternative idea is to approximate the Q-table with a function. Suppose you have a state space of around 106 states and action space is continuous value from [0,1]. Instead of trying to represent the Q table explicitly, we can approximate the table with a function.  While function approximation is almost as old as theory of RL itself, it recently regained popularity due to a seminal paper on a Neural Network based Q-learning algorithm called the DQN algorithm, which was proposed in 2015.\n",
    "\n",
    "In this MP, you have been asked to implement the Deep Q-learning algorithm (DQN).  The idea of this algorithm is to learn a sufficiently close approximation to the optimal Q-function using a Neural Network architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN architecture\n",
    "In the cell below is the Neural Network architecture for Deep Q-learning for this MP. You don't have to implement anything here but please go through the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as weight_init\n",
    "\n",
    "class QFunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, learning_rate, epsilon, seed, batch_size, tau, duel_enable = False, duel_type = 'avg'):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate \n",
    "        self.epsilon = epsilon\n",
    "        self.seed = seed\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        super(QFunction, self).__init__()\n",
    "        # Define the two layered network\n",
    "        self.layer1 = nn.Linear(self.state_dim,48)\n",
    "        n = weight_init._calculate_fan_in_and_fan_out(self.layer1.weight)[0]\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.layer1.weight.data.uniform_(-math.sqrt(6./n), math.sqrt(6./n))\n",
    "        \n",
    "        self.layer2 = nn.Linear(48,action_dim)\n",
    "        n = weight_init._calculate_fan_in_and_fan_out(self.layer2.weight)[0]\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.layer2.weight.data.uniform_(-math.sqrt(6./n), math.sqrt(6./n))\n",
    "        \n",
    "        # Define the loss function and the optimizer that is being used\n",
    "        self.loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay = 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.layer1(x))\n",
    "        y = self.layer2(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "    def train(self, states, actions, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        q_value = self.forward(states)\n",
    "        actions = actions.data.numpy().astype(int)\n",
    "        range_array = np.array(range(self.batch_size))\n",
    "        index_range = np.arange(self.batch_size)\n",
    "        index_range = np.reshape(index_range,(1, self.batch_size))\n",
    "        q_value = q_value[index_range, actions]\n",
    "        loss = self.loss_fn(q_value,y)\n",
    "        # print('loss:', loss)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def update_target_weights(self, critic):\n",
    "\n",
    "        for weight,target_weight in zip(self.parameters(),critic.parameters()):\n",
    "            weight.data = (1-self.tau)*weight.data +  (self.tau)*target_weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "In the next cell we have defined replay buffer. Again, you don't have to implement anything here but please go through the code for the buffer because it is used in almost all Deep Reinforcement learning applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code for the ReplayBuffer\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, buffer_size, random_seed=None):\n",
    "        \"\"\"\n",
    "        The right side of the deque contains the most recent experiences\n",
    "        The buffer stores a number of past experiences to stochastically sample from\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "        self.seed = random_seed\n",
    "        if self.seed is not None:\n",
    "            random.seed(self.seed)\n",
    "\n",
    "    def add(self, state, action, reward, t, s2):\n",
    "        experience = (state, action, reward, t, s2)\n",
    "        self.buffer.append(experience)\n",
    "        self.count += 1\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch]).reshape(batch_size, -1)\n",
    "        t_batch = np.array([_[3] for _ in batch]).reshape(batch_size, -1)\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "In the next cell; you are going to implement the DQN learning algorithm. Specifically, you are implementing the epsilon-greedy policy and the weight update parts of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a788920f6083>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as weight_init\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class DDQNAgent:\n",
    "    def __init__(self, critic, replay_buffer, episode_len=1000, episode_steps=1000, epsilon=0.01, epsilon_decay=0.999,\n",
    "                 batch_size=64, gamma=0.99, seed=1234):\n",
    "        self.critic = copy.deepcopy(critic)\n",
    "        self.target_critic = copy.deepcopy(critic)\n",
    "        self.replay_buffer = copy.deepcopy(replay_buffer)\n",
    "        self.episode_len = episode_len\n",
    "        self.episode_steps = episode_steps\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.seed = seed\n",
    "\n",
    "    def take_action(self, state, n_actions, epsilon_t):\n",
    "        random_num = np.random.uniform(0, 1)\n",
    "        ##########\n",
    "        # epsilon_t = 0\n",
    "        ##########\n",
    "        if random_num < epsilon_t:\n",
    "            # act randomly\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            values = self.critic.forward(state)\n",
    "            values = values.cpu().detach().numpy()  # values = [[float, float, ..., float]]\n",
    "            return np.argmax(values[0])\n",
    "\n",
    "    def train(self, env):\n",
    "        CUDA = torch.cuda.is_available()\n",
    "        epsilon_t = 1.0\n",
    "        for i in range(self.episode_len):\n",
    "            j = 0\n",
    "            state = time()\n",
    "            env.seed(self.seed + i)\n",
    "            s = env.reset()\n",
    "            terminal = False\n",
    "            clear_output(wait=True)\n",
    "            print(\"iterations\", i)\n",
    "            while not terminal:\n",
    "\n",
    "                j = j + 1\n",
    "                input_state = np.reshape(s, (1, self.critic.state_dim))\n",
    "                input_state = torch.from_numpy(input_state)\n",
    "                dtype = torch.FloatTensor\n",
    "                input_state = Variable(input_state.type(dtype), requires_grad=True)\n",
    "                if CUDA:\n",
    "                    input_state = input_state.cuda()\n",
    "\n",
    "                a = self.take_action(input_state, env.action_space.n, epsilon_t)\n",
    "                # print('action taken:', a)\n",
    "                s2, r, terminal, info = env.step(a)\n",
    "\n",
    "                self.replay_buffer.add(np.reshape(s, (self.critic.state_dim,)),\n",
    "                                       a,\n",
    "                                       r, terminal, np.reshape(s2, (self.critic.state_dim,)))\n",
    "                # epsilon-greedy policy\n",
    "                if epsilon_t > self.epsilon:\n",
    "                    epsilon_t = epsilon_t * self.epsilon_decay\n",
    "                else:\n",
    "                    epsilon_t = self.epsilon\n",
    "                if self.replay_buffer.size() > self.batch_size:\n",
    "                    s_batch, a_batch, r_batch, t_batch, s2_batch = self.replay_buffer.sample_batch(self.batch_size)\n",
    "\n",
    "                    s2_batch = torch.from_numpy(s2_batch)\n",
    "                    s2_batch = Variable(s2_batch.type(torch.FloatTensor), requires_grad=False)\n",
    "\n",
    "                    s_batch = torch.from_numpy(s_batch).type(torch.FloatTensor)\n",
    "                    a_batch = torch.from_numpy(a_batch).type(torch.FloatTensor)\n",
    "                    r_batch = torch.from_numpy(r_batch).type(torch.FloatTensor)\n",
    "\n",
    "                    # find y:\n",
    "                    q_values = self.target_critic.forward(s2_batch)\n",
    "                    maxQ = torch.max(q_values, 1)[0]\n",
    "                    maxQ = torch.reshape(maxQ, r_batch.shape)\n",
    "                    assert (r_batch.shape == maxQ.shape)\n",
    "                    y = r_batch + self.gamma * maxQ\n",
    "\n",
    "                    # print(t_batch)\n",
    "                    for i in range(len(y[0])):\n",
    "                        if t_batch[i, 0]:\n",
    "                            y[i, 0] = r_batch[i, 0]\n",
    "\n",
    "\n",
    "                    if CUDA:\n",
    "                        s2_batch = s2_batch.cuda()\n",
    "                        s_batch = s_batch.cuda()\n",
    "                        a_batch = a_batch.cuda()\n",
    "                        y = y.cuda()\n",
    "                    ######################\n",
    "                    # Add your code here\n",
    "                    # You have to write the\n",
    "                    # gradient step of the Q-Function here.\n",
    "                    #####################\n",
    "\n",
    "                    self.critic.train(s_batch, a_batch, y)\n",
    "                    if i % 1 == 0:\n",
    "                        self.target_critic.update_target_weights(self.critic)\n",
    "\n",
    "                else:\n",
    "                    loss = 0\n",
    "                if j > 100:\n",
    "                    terminal = True\n",
    "\n",
    "                s = s2\n",
    "\n",
    "    def test(self, env):\n",
    "        print ('##########Test begins!#########')\n",
    "        total_epochs, total_penalties = 0, 0\n",
    "        episodes = 1\n",
    "        total_reward = 0\n",
    "        CUDA = torch.cuda.is_available()\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = time()\n",
    "            s = env.reset()\n",
    "            epochs, penalties, reward = 0, 0, 0\n",
    "            print(\"iterations\", i)\n",
    "            j = 0\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                env.render()\n",
    "                j += 1\n",
    "                input_state = np.reshape(s, (1, self.critic.state_dim))\n",
    "                input_state = torch.from_numpy(input_state)\n",
    "                dtype = torch.FloatTensor\n",
    "                input_state = Variable(input_state.type(dtype), requires_grad=True)\n",
    "\n",
    "                if CUDA:\n",
    "                    input_state = input_state.cuda()\n",
    "\n",
    "                a = self.critic(input_state)\n",
    "                a = a.data.cpu().numpy()\n",
    "                a = np.argmax(a)\n",
    "                s, r, terminal, info = env.step(a)\n",
    "                total_reward += r\n",
    "                if r == 20:\n",
    "                    print(\"dropped off\")\n",
    "                if r == -10:\n",
    "                    penalties += 1\n",
    "                epochs += 1\n",
    "                if terminal or j > 100:\n",
    "                    break\n",
    "\n",
    "            total_penalties += penalties\n",
    "            total_epochs += epochs\n",
    "\n",
    "        print(\"Results after {episodes} episodes:\")\n",
    "        print(\"Total reward: \", total_reward)\n",
    "        print(\"Average timesteps per episode:\", total_epochs / episodes)\n",
    "        print(\"Average penalties per episode:\", total_penalties / episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class arg:\n",
    "    def __init__(self):\n",
    "        self.seed = 1234\n",
    "        self.tau = 0.001\n",
    "        self.learning_rate =0.01\n",
    "        self.batch_size=64\n",
    "        self.bufferlength=2000\n",
    "        self.l2_decay=0.01\n",
    "        self.gamma=0.6\n",
    "        self.episode_len=500\n",
    "        self.episode_steps=1000\n",
    "        self.epsilon=0.1\n",
    "        self.epsilon_decay=0.999\n",
    "        self.is_train=True\n",
    "        self.actor_weights='ddqn_cartpole'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DQN implementation\n",
    "Test your DQN impelementation. It first trains the Q-networks and then proceeds to test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from time import time\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as weight_init\n",
    "\n",
    "def main(args):\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    \n",
    "    env = gym.make(\"Taxi-v2\").env\n",
    "    state_dim = 1\n",
    "    action_dim = 6\n",
    "\n",
    "    qfunction = QFunction(state_dim, action_dim, learning_rate = args.learning_rate, epsilon = args.epsilon, seed = args.seed, batch_size = args.batch_size, tau = args.tau)\n",
    "\n",
    "    if CUDA: \n",
    "        qfunction = qfunction.cuda()\n",
    "\n",
    "    replay_buffer = ReplayBuffer(args.bufferlength)\n",
    "\n",
    "    agent = DDQNAgent(qfunction, replay_buffer, episode_len = args.episode_len,\n",
    "                     episode_steps=args.episode_steps, epsilon = args.epsilon, epsilon_decay = args.epsilon_decay,\n",
    "                     batch_size = args.batch_size, gamma = args.gamma, seed = args.seed)\n",
    "\n",
    "    if args.is_train:\n",
    "        agent.train(env)\n",
    "        agent.test(env)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = arg()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
