{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install cmake 'gym[atari]' scipy numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules imported!\n"
     ]
    }
   ],
   "source": [
    "#Understand the **Taxi-v2** environment in gym. We first create the environment\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print (\"modules imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create the taxi-v2 environment\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "\n",
    "env.render() #creates the simulation in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space:  Discrete(6)\n",
      "State Space:  Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space: \", env.action_space) #action space of the Taxi-environment\n",
    "print(\"State Space: \", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first understand the state-space of this problem. We define the state of the taxi with the following co-ordinates \n",
    "\n",
    "(row, column, passenger_index, destination_index)\n",
    "\n",
    "where, *row* and *column* denotes the location of the taxi in a 5X5 environment. \n",
    "*passenger_index* : is the index number of passenger in the taxi. It could either be empty(0) or filled with either one of the passengers(1,2,3,4). Similarly, destination index is one of the four locations of the taxi(1,2,3,4) for one of the four locations. Thus, the total possible state space of the problem is as follows, 5X5X5X4 = 500\n",
    "\n",
    "The action space of this algorithm has six values. Of these the first four values denote the direction of car's going. The next two action space is to either pick-up or drop-off the car at the particular location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "encoded state 328\n",
      "decoded_state [3 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "state_e = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "state_d = env.decode(state_e)\n",
    "env.render()\n",
    "print (\"encoded state\", state_e)\n",
    "print (\"decoded_state\", np.array(list(state_d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 1112\n",
      "Penalties incurred: 351\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Let's now, run a policy wherein the actions are randomly choosen\n",
    "to get an idea how good or worse a random policy performs and the \n",
    "'''\n",
    "env.reset()  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken:\", format(epochs))\n",
    "print(\"Penalties incurred:\",format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning Algorithm\n",
    "In the next cell. You are required to implement the Q-learning algorithm. \n",
    "\n",
    "This has been described in MP4 write up as well as in class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q-learning algorithm implementation\n",
    "# Set Hyper-parameters for the Q-learning algorithm\n",
    "alpha = 0.1 # learning rate\n",
    "gamma = 0.6\n",
    "epsilon = 0.1 #epsilon value for epsilon-greedy policy\n",
    "# Initialise the Q-table for the problem\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "# Survelliance data for the whole run of this algorithm\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "M = 100001 # episodic runs of the Q-learning algorithm\n",
    "for i in range(1, M):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        ###\n",
    "        #Add you code here. \n",
    "        #Remember action in line 24 would be based on the epsilon-greedy policy\n",
    "        #described above\n",
    "        ###\n",
    "        random_num = np.random.uniform(0, 1)\n",
    "        if random_num < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, info = env.step(action) #next state\n",
    "        #env.render() #render the simulation to see it on the screen\n",
    "        \n",
    "\n",
    "        ###\n",
    "        # Add the remaining part of the code here\n",
    "        # Update Q table after taking action above\n",
    "        ###\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        q_table[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        \n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        if epochs > 100:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Episode:\", i)\n",
    "\n",
    "print(\"Training finished.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Test your Q-learning implementation in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after {episodes} episodes:\n",
      "Average timesteps per episode: 12.63\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Testing Q-learning implementation\n",
    "def test(q_table, episodes = 100):\n",
    "    total_epochs, total_penalties = 0, 0\n",
    "    #episodes = 100\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        epochs, penalties, reward = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state])\n",
    "            state, reward, done, info = env.step(action)\n",
    "            #print (state)\n",
    "            #env.render()\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "\n",
    "            epochs += 1\n",
    "            \n",
    "            if epochs > 100:\n",
    "                break\n",
    "\n",
    "        total_penalties += penalties\n",
    "        total_epochs += epochs\n",
    "\n",
    "    print(\"Results after {episodes} episodes:\")\n",
    "    print(\"Average timesteps per episode:\", total_epochs/episodes)\n",
    "    print(\"Average penalties per episode:\", total_penalties/episodes)\n",
    "test(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Function Approximations in Reinforcement learning\n",
    "You will observe that training a simple environment with state, action pairs of 500×6 elements takes a long time to converge using Q-learning. Additionally, you have to create 500×6 element size Q-table to store all the Q-values. This process of making such a large Q-table is memory intensive. You also may have noticed that this method of using a Q-table only works when the state and action spaces is discrete. These practical issues with Q-learning was what originally prevented it being from used in many real-world scenarios.\n",
    "\n",
    "If the state space/action spaces are large, an alternative idea is to approximate the Q-table with a function. Suppose you have a state space of around 106 states and action space is continuous value from [0,1]. Instead of trying to represent the Q table explicitly, we can approximate the table with a function.  While function approximation is almost as old as theory of RL itself, it recently regained popularity due to a seminal paper on a Neural Network based Q-learning algorithm called the DQN algorithm, which was proposed in 2015.\n",
    "\n",
    "In this MP, you have been asked to implement the Deep Q-learning algorithm (DQN).  The idea of this algorithm is to learn a sufficiently close approximation to the optimal Q-function using a Neural Network architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN architecture\n",
    "In the cell below is the Neural Network architecture for Deep Q-learning for this MP. You don't have to implement anything here but please go through the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as weight_init\n",
    "\n",
    "class QFunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, learning_rate, epsilon, seed, batch_size, tau, duel_enable = False, duel_type = 'avg'):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate \n",
    "        self.epsilon = epsilon\n",
    "        self.seed = seed\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        super(QFunction, self).__init__()\n",
    "        # Define the two layered network\n",
    "        self.layer1 = nn.Linear(self.state_dim,48)\n",
    "        n = weight_init._calculate_fan_in_and_fan_out(self.layer1.weight)[0]\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.layer1.weight.data.uniform_(-math.sqrt(6./n), math.sqrt(6./n))\n",
    "        \n",
    "        self.layer2 = nn.Linear(48,action_dim)\n",
    "        n = weight_init._calculate_fan_in_and_fan_out(self.layer2.weight)[0]\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.layer2.weight.data.uniform_(-math.sqrt(6./n), math.sqrt(6./n))\n",
    "        \n",
    "        # Define the loss function and the optimizer that is being used\n",
    "        self.loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay = 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.layer1(x))\n",
    "        y = self.layer2(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "    def train(self, states, actions, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        q_value = self.forward(states)\n",
    "        actions = actions.data.numpy().astype(int)\n",
    "        range_array = np.array(range(self.batch_size))\n",
    "        index_range = np.arange(self.batch_size)\n",
    "        index_range = np.reshape(index_range,(1, self.batch_size))\n",
    "        q_value = q_value[index_range, actions]\n",
    "        loss = self.loss_fn(q_value,y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def update_target_weights(self, critic):\n",
    "\n",
    "        for weight,target_weight in zip(self.parameters(),critic.parameters()):\n",
    "            weight.data = (1-self.tau)*weight.data +  (self.tau)*target_weight.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "In the next cell we have defined replay buffer. Again, you don't have to implement anything here but please go through the code for the buffer because it is used in almost all Deep Reinforcement learning applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code for the ReplayBuffer\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, buffer_size, random_seed=None):\n",
    "        \"\"\"\n",
    "        The right side of the deque contains the most recent experiences\n",
    "        The buffer stores a number of past experiences to stochastically sample from\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "        self.seed = random_seed\n",
    "        if self.seed is not None:\n",
    "            random.seed(self.seed)\n",
    "\n",
    "    def add(self, state, action, reward, t, s2):\n",
    "        experience = (state, action, reward, t, s2)\n",
    "        self.buffer.append(experience)\n",
    "        self.count += 1\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch]).reshape(batch_size, -1)\n",
    "        t_batch = np.array([_[3] for _ in batch]).reshape(batch_size, -1)\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "In the next cell; you are going to implement the DQN learning algorithm. Specifically, you are implementing the epsilon-greedy policy and the weight update parts of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as weight_init\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class DDQNAgent: \n",
    "    def __init__(self, critic, replay_buffer, episode_len = 1000, episode_steps=1000, epsilon = 0.01, epsilon_decay = 0.999, batch_size = 64, gamma = 0.99, seed = 1234):\n",
    "        self.critic = copy.deepcopy(critic)\n",
    "        self.target_critic = copy.deepcopy(critic)\n",
    "        self.replay_buffer = copy.deepcopy(replay_buffer)\n",
    "        self.episode_len = episode_len\n",
    "        self.episode_steps = episode_steps\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.seed = seed\n",
    "\n",
    "    def take_action(self, state, n_actions, epsilon_t):\n",
    "        random_num = np.random.uniform(0, 1)\n",
    "        ##########\n",
    "        # epsilon_t = 0\n",
    "        ##########\n",
    "        if random_num < epsilon_t:\n",
    "            # act randomly\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            # act greedily according to self.critic\n",
    "            values = self.critic(state)\n",
    "            values = values.cpu().detach().numpy() # values = [[float, float, ..., float]]\n",
    "            return np.argmax(values[0])\n",
    "\n",
    "    def train(self, env):\n",
    "        CUDA = torch.cuda.is_available()\n",
    "        epsilon_t = 1.0\n",
    "        for i in range(self.episode_len):\n",
    "            j = 0\n",
    "            state = time()\n",
    "            env.seed(self.seed + i)\n",
    "            s = env.reset()\n",
    "            terminal = False\n",
    "            clear_output(wait=True)\n",
    "            print (\"iterations\", i)\n",
    "            while not terminal:\n",
    "                j = j + 1\n",
    "                #print (\"J\", j)\n",
    "                #env.render()\n",
    "                input_state  = np.reshape(s, (1, self.critic.state_dim))\n",
    "                input_state = torch.from_numpy(input_state)\n",
    "                dtype = torch.FloatTensor\n",
    "                input_state = Variable(input_state.type(dtype),requires_grad=True)\n",
    "                if CUDA:\n",
    "                    input_state = input_state.cuda()\n",
    "\n",
    "#                 a = a.data.cpu().numpy()\n",
    "#                 a = self.take_action(a[0], env.action_space.n, epsilon_t)\n",
    "                a = self.take_action(input_state, env.action_space.n, epsilon_t)\n",
    "                s2, r, terminal, info = env.step(a)\n",
    "                self.replay_buffer.add(np.reshape(s, (self.critic.state_dim,)),\n",
    "                                  a,\n",
    "                                  r, terminal, np.reshape(s2, (self.critic.state_dim,)))\n",
    "                # epsilon-greedy policy\n",
    "                if epsilon_t > self.epsilon:\n",
    "                    epsilon_t = epsilon_t*self.epsilon_decay\n",
    "                else : \n",
    "                    epsilon_t = self.epsilon\n",
    "                if self.replay_buffer.size() > self.batch_size:\n",
    "                    s_batch, a_batch, r_batch, t_batch, s2_batch = self.replay_buffer.sample_batch(self.batch_size)\n",
    "                    s2_batch = torch.from_numpy(s2_batch)\n",
    "                    s2_batch = Variable(s2_batch.type(torch.FloatTensor),requires_grad=False)\n",
    "                    \n",
    "                    if CUDA:\n",
    "                        s2_batch = s2_batch.cuda()\n",
    "                    ######################\n",
    "                    # Add your code here     \n",
    "                    # You have to write the \n",
    "                    # gradient step of the Q-Function here.\n",
    "                    #####################\n",
    "                    s_batch = torch.from_numpy(s_batch).type(torch.FloatTensor)\n",
    "                    a_batch = torch.from_numpy(a_batch).type(torch.FloatTensor)\n",
    "                    y = s2_batch * self.gamma + torch.from_numpy(r_batch).type(torch.FloatTensor)\n",
    "                    # self.critic.train(s_batch, a_batch, y)\n",
    "                    \n",
    "                    if CUDA: \n",
    "                        s_batch = s_batch.cuda()\n",
    "                        a_batch = a_batch.cuda()\n",
    "                        y = y.cuda()\n",
    "                    self.critic.train(s_batch, a_batch, y)\n",
    "                    if i % 10 == 0:\n",
    "                        self.target_critic.update_target_weights(self.critic)\n",
    "\n",
    "                else:\n",
    "                    loss = 0\n",
    "                if j > 100:\n",
    "                    terminal = True\n",
    "\n",
    "                s = s2\n",
    "\n",
    "    def test(self, env):\n",
    "        total_epochs, total_penalties = 0, 0\n",
    "        episodes = 100\n",
    "        CUDA = torch.cuda.is_available()\n",
    "        for i in range(episodes):\n",
    "            state = time()\n",
    "            s = env.reset()\n",
    "            epochs, penalties, reward = 0, 0, 0\n",
    "            print (\"iterations\", i)\n",
    "            j = 0\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                # env.render()\n",
    "                j += 1\n",
    "                input_state  = np.reshape(s, (1, self.critic.state_dim))\n",
    "                input_state = torch.from_numpy(input_state)\n",
    "                dtype = torch.FloatTensor\n",
    "                input_state = Variable(input_state.type(dtype),requires_grad=True)\n",
    "\n",
    "                if CUDA:\n",
    "                    input_state = input_state.cuda()\n",
    "\n",
    "                a = self.critic(input_state) \n",
    "                a = a.data.cpu().numpy()\n",
    "                a = np.argmax(a)\n",
    "                s, r, terminal, info = env.step(a)\n",
    "                if r == -10:\n",
    "                    penalties += 1\n",
    "                epochs += 1\n",
    "                if terminal or j > 100:\n",
    "                    break\n",
    "\n",
    "\n",
    "            total_penalties += penalties\n",
    "            total_epochs += epochs\n",
    "\n",
    "        print(\"Results after {episodes} episodes:\")\n",
    "        print(\"Average timesteps per episode:\", total_epochs/episodes)\n",
    "        print(\"Average penalties per episode:\", total_penalties/episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class arg:\n",
    "    def __init__(self):\n",
    "        self.seed = 1234\n",
    "        self.tau = 0.001\n",
    "        self.learning_rate =0.001\n",
    "        self.batch_size=64\n",
    "        self.bufferlength=2000\n",
    "        self.l2_decay=0.01\n",
    "        self.gamma=0.6\n",
    "        self.episode_len=100\n",
    "        self.episode_steps=1000\n",
    "        self.epsilon=0.01\n",
    "        self.epsilon_decay=0.999\n",
    "        self.is_train=True\n",
    "        self.actor_weights='ddqn_cartpole'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DQN implementation\n",
    "Test your DQN impelementation. It first trains the Q-networks and then proceeds to test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations 99\n",
      "iterations 0\n",
      "iterations 1\n",
      "iterations 2\n",
      "iterations 3\n",
      "iterations 4\n",
      "iterations 5\n",
      "iterations 6\n",
      "iterations 7\n",
      "iterations 8\n",
      "iterations 9\n",
      "iterations 10\n",
      "iterations 11\n",
      "iterations 12\n",
      "iterations 13\n",
      "iterations 14\n",
      "iterations 15\n",
      "iterations 16\n",
      "iterations 17\n",
      "iterations 18\n",
      "iterations 19\n",
      "iterations 20\n",
      "iterations 21\n",
      "iterations 22\n",
      "iterations 23\n",
      "iterations 24\n",
      "iterations 25\n",
      "iterations 26\n",
      "iterations 27\n",
      "iterations 28\n",
      "iterations 29\n",
      "iterations 30\n",
      "iterations 31\n",
      "iterations 32\n",
      "iterations 33\n",
      "iterations 34\n",
      "iterations 35\n",
      "iterations 36\n",
      "iterations 37\n",
      "iterations 38\n",
      "iterations 39\n",
      "iterations 40\n",
      "iterations 41\n",
      "iterations 42\n",
      "iterations 43\n",
      "iterations 44\n",
      "iterations 45\n",
      "iterations 46\n",
      "iterations 47\n",
      "iterations 48\n",
      "iterations 49\n",
      "iterations 50\n",
      "iterations 51\n",
      "iterations 52\n",
      "iterations 53\n",
      "iterations 54\n",
      "iterations 55\n",
      "iterations 56\n",
      "iterations 57\n",
      "iterations 58\n",
      "iterations 59\n",
      "iterations 60\n",
      "iterations 61\n",
      "iterations 62\n",
      "iterations 63\n",
      "iterations 64\n",
      "iterations 65\n",
      "iterations 66\n",
      "iterations 67\n",
      "iterations 68\n",
      "iterations 69\n",
      "iterations 70\n",
      "iterations 71\n",
      "iterations 72\n",
      "iterations 73\n",
      "iterations 74\n",
      "iterations 75\n",
      "iterations 76\n",
      "iterations 77\n",
      "iterations 78\n",
      "iterations 79\n",
      "iterations 80\n",
      "iterations 81\n",
      "iterations 82\n",
      "iterations 83\n",
      "iterations 84\n",
      "iterations 85\n",
      "iterations 86\n",
      "iterations 87\n",
      "iterations 88\n",
      "iterations 89\n",
      "iterations 90\n",
      "iterations 91\n",
      "iterations 92\n",
      "iterations 93\n",
      "iterations 94\n",
      "iterations 95\n",
      "iterations 96\n",
      "iterations 97\n",
      "iterations 98\n",
      "iterations 99\n",
      "Results after {episodes} episodes:\n",
      "Average timesteps per episode: 101.0\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from time import time\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as weight_init\n",
    "\n",
    "def main(args):\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    \n",
    "    env = gym.make(\"Taxi-v2\").env\n",
    "    state_dim = 1\n",
    "    action_dim = 6\n",
    "\n",
    "    qfunction = QFunction(state_dim, action_dim, learning_rate = args.learning_rate, epsilon = args.epsilon, seed = args.seed, batch_size = args.batch_size, tau = args.tau)\n",
    "\n",
    "    if CUDA: \n",
    "        qfunction = qfunction.cuda()\n",
    "\n",
    "    replay_buffer = ReplayBuffer(args.bufferlength)\n",
    "\n",
    "    agent = DDQNAgent(qfunction, replay_buffer, episode_len = args.episode_len,\n",
    "                     episode_steps=args.episode_steps, epsilon = args.epsilon, epsilon_decay = args.epsilon_decay,\n",
    "                     batch_size = args.batch_size, gamma = args.gamma, seed = args.seed)\n",
    "\n",
    "    if args.is_train:\n",
    "        agent.train(env)\n",
    "        agent.test(env)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = arg()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
