{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cmake 'gym[atari]' scipy numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand the **Taxi-v2** environment in gym. We first create the environment\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print (\"modules imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the taxi-v2 environment\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "\n",
    "env.render() #creates the simulation in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space: \", env.action_space) #action space of the Taxi-environment\n",
    "print(\"State Space: \", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first understand the state-space of this problem. We define the state of the taxi with the following co-ordinates \n",
    "```\n",
    "(row, column, passenger_index, destination_index)\n",
    "```\n",
    "where, *row* and *column* denotes the location of the taxi in a 5X5 environment. \n",
    "*passenger_index* : is the index number of passenger in the taxi. It could either be empty(0) or filled with either one of the passengers(1,2,3,4). Similarly, destination index is one of the four locations of the taxi(1,2,3,4) for one of the four locations. Thus, the total possible state space of the problem is as follows, $5X5X5X4 = 500$\n",
    "\n",
    "The action space of this algorithm has six values. Of these the first four values denote the direction of car's going. The next two action space is to either pick-up or drop-off the car at the particular location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_e = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "state_d = env.decode(state_e)\n",
    "env.render()\n",
    "print (\"encoded state\", state_e)\n",
    "print (\"decoded_state\", np.array(list(state_d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's now, run a policy wherein the actions are randomly choosen\n",
    "to get an idea how good or worse a random policy performs and the \n",
    "'''\n",
    "env.reset()  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken:\", format(epochs))\n",
    "print(\"Penalties incurred:\",format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning algorithm implementation\n",
    "# Set Hyper-parameters for the Q-learning algorithm\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "# Initialise the Q-table for the problem\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "# Survelliance data for the whole run of this algorithm\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "M = 100001 # episodic runs of the Q-learning algorithm\n",
    "for i in range(1, M):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        ###\n",
    "        #Add you code here. \n",
    "        #Remember action in line 24 would be based on the epsilon-greedy policy\n",
    "        #described above\n",
    "        ###\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action) #next state\n",
    "        #env.render() #render the simulation to see it on the screen\n",
    "        \n",
    "\n",
    "        ###\n",
    "        # Add the remaining part of the code here\n",
    "        # Update Q table after taking action above\n",
    "        ###\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        if epochs > 100:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Episode:\", i)\n",
    "\n",
    "print(\"Training finished.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Q-learning implementation\n",
    "def test(q_table, episodes = 100):\n",
    "    total_epochs, total_penalties = 0, 0\n",
    "    #episodes = 100\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        epochs, penalties, reward = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state])\n",
    "            state, reward, done, info = env.step(action)\n",
    "            #print (state)\n",
    "            #env.render()\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "\n",
    "            epochs += 1\n",
    "            \n",
    "            if epochs > 100:\n",
    "                break\n",
    "\n",
    "        total_penalties += penalties\n",
    "        total_epochs += epochs\n",
    "\n",
    "    print(\"Results after {episodes} episodes:\")\n",
    "    print(\"Average timesteps per episode:\", total_epochs/episodes)\n",
    "    print(\"Average penalties per episode:\", total_penalties/episodes)\n",
    "test(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as weight_init\n",
    "\n",
    "class QFunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, learning_rate, epsilon, seed, batch_size, tau, duel_enable = False, duel_type = 'avg'):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate \n",
    "        self.epsilon = epsilon\n",
    "        self.seed = seed\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        super(QFunction, self).__init__()\n",
    "        # Define the two layered network\n",
    "        self.layer1 = nn.Linear(self.state_dim,48)\n",
    "        n = weight_init._calculate_fan_in_and_fan_out(self.layer1.weight)[0]\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.layer1.weight.data.uniform_(-math.sqrt(6./n), math.sqrt(6./n))\n",
    "        \n",
    "        self.layer2 = nn.Linear(48,action_dim)\n",
    "        n = weight_init._calculate_fan_in_and_fan_out(self.layer2.weight)[0]\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.layer2.weight.data.uniform_(-math.sqrt(6./n), math.sqrt(6./n))\n",
    "        \n",
    "        # Define the loss function and the optimizer that is being used\n",
    "        self.loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay = 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.layer1(x))\n",
    "        y = self.layer2(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "    def train(self, states, actions, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        q_value = self.forward(states)\n",
    "        actions = actions.data.numpy().astype(int)\n",
    "        range_array = np.array(range(self.batch_size))\n",
    "        index_range = np.arange(self.batch_size)\n",
    "        index_range = np.reshape(index_range,(1, self.batch_size))\n",
    "        q_value = q_value[index_range, actions]\n",
    "        loss = self.loss_fn(q_value,y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def update_target_weights(self, critic):\n",
    "\n",
    "        for weight,target_weight in zip(self.parameters(),critic.parameters()):\n",
    "            weight.data = (1-self.tau)*weight.data +  (self.tau)*target_weight.data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for the ReplayBuffer\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, buffer_size, random_seed=None):\n",
    "        \"\"\"\n",
    "        The right side of the deque contains the most recent experiences\n",
    "        The buffer stores a number of past experiences to stochastically sample from\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "        self.seed = random_seed\n",
    "        if self.seed is not None:\n",
    "            random.seed(self.seed)\n",
    "\n",
    "    def add(self, state, action, reward, t, s2):\n",
    "        experience = (state, action, reward, t, s2)\n",
    "        self.buffer.append(experience)\n",
    "        self.count += 1\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch]).reshape(batch_size, -1)\n",
    "        t_batch = np.array([_[3] for _ in batch]).reshape(batch_size, -1)\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as weight_init\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class DDQNAgent: \n",
    "    def __init__(self, critic, replay_buffer, episode_len = 1000, episode_steps=1000, epsilon = 0.01, epsilon_decay = 0.999, batch_size = 64, gamma = 0.99, seed = 1234):\n",
    "        self.critic = copy.deepcopy(critic)\n",
    "        self.target_critic = copy.deepcopy(critic)\n",
    "        self.replay_buffer = copy.deepcopy(replay_buffer)\n",
    "        self.episode_len = episode_len\n",
    "        self.episode_steps = episode_steps\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.seed = seed\n",
    "\n",
    "    def take_action(self, action, nb_actions, epsilon_t):\n",
    "        ## Write code here for epsilon-greedy policy\n",
    "        # output_action = np.ones(nb_actions)\n",
    "        # output_prob = output_action*epsilon_t*(1./nb_actions)\n",
    "        # max_argument = np.argmax(action)\n",
    "        # output_prob[max_argument] += 1 - epsilon_t\n",
    "        return None\n",
    "\n",
    "\n",
    "    def train(self, env):\n",
    "        CUDA = torch.cuda.is_available()\n",
    "        epsilon_t = 1.0\n",
    "        for i in range(self.episode_len):\n",
    "            j = 0\n",
    "            state = time()\n",
    "            env.seed(self.seed + i)\n",
    "            s = env.reset()\n",
    "            terminal = False\n",
    "            clear_output(wait=True)\n",
    "            print (\"iterations\", i)\n",
    "            while not terminal:\n",
    "                j = j + 1\n",
    "                #print (\"J\", j)\n",
    "                #env.render()\n",
    "                input_state  = np.reshape(s, (1, self.critic.state_dim))\n",
    "                input_state = torch.from_numpy(input_state)\n",
    "                dtype = torch.FloatTensor\n",
    "                input_state = Variable(input_state.type(dtype),requires_grad=True)\n",
    "                if CUDA:\n",
    "                    input_state = input_state.cuda()\n",
    "                a = self.critic(input_state) \n",
    "                a = a.data.cpu().numpy()\n",
    "                a = self.take_action(a[0], env.action_space.n, epsilon_t)\n",
    "                s2, r, terminal, info = env.step(a)\n",
    "                self.replay_buffer.add(np.reshape(s, (self.critic.state_dim,)),\n",
    "                                  a,\n",
    "                                  r, terminal, np.reshape(s2, (self.critic.state_dim,)))\n",
    "                # epsilon-greedy policy\n",
    "                if epsilon_t > self.epsilon:\n",
    "                    epsilon_t = epsilon_t*self.epsilon_decay\n",
    "                else : \n",
    "                    epsilon_t = self.epsilon\n",
    "                if self.replay_buffer.size() > self.batch_size:\n",
    "                    s_batch, a_batch, r_batch, t_batch, s2_batch = self.replay_buffer.sample_batch(self.batch_size)\n",
    "                    s2_batch = torch.from_numpy(s2_batch)\n",
    "                    s2_batch = Variable(s2_batch.type(torch.FloatTensor),requires_grad=False)\n",
    "                    if CUDA:\n",
    "                        s2_batch = s2_batch.cuda()\n",
    "                    ######################\n",
    "                    # Add your code here     \n",
    "                    # You have to write the \n",
    "                    # gradient step of the Q-Function here.\n",
    "                    #####################\n",
    "                    if CUDA: \n",
    "                        s_batch = s_batch.cuda()\n",
    "                        a_batch = a_batch.cuda()\n",
    "                        y = y.cuda()\n",
    "                    self.critic.train(s_batch, a_batch, y)\n",
    "                    #self.critic.update_target_weights(critic)\n",
    "\n",
    "                else:\n",
    "                    loss = 0\n",
    "                if j > 100:\n",
    "                    terminal = True\n",
    "\n",
    "                s = s2\n",
    "\n",
    "    def test(self, env):\n",
    "        total_epochs, total_penalties = 0, 0\n",
    "        episodes = 100\n",
    "        CUDA = torch.cuda.is_available()\n",
    "        for i in range(episodes):\n",
    "            state = time()\n",
    "            s = env.reset()\n",
    "            epochs, penalties, reward = 0, 0, 0\n",
    "            print (\"iterations\", i)\n",
    "            j = 0\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                env.render()\n",
    "                j += 1\n",
    "                input_state  = np.reshape(s, (1, self.critic.state_dim))\n",
    "                input_state = torch.from_numpy(input_state)\n",
    "                dtype = torch.FloatTensor\n",
    "                input_state = Variable(input_state.type(dtype),requires_grad=True)\n",
    "\n",
    "                if CUDA:\n",
    "                    input_state = input_state.cuda()\n",
    "\n",
    "                a = self.critic(input_state) \n",
    "                a = a.data.cpu().numpy()\n",
    "                a = np.argmax(a)\n",
    "                s, r, terminal, info = env.step(a)\n",
    "                if r == -10:\n",
    "                    penalties += 1\n",
    "                epochs += 1\n",
    "                if terminal or j > 100:\n",
    "                    break\n",
    "\n",
    "\n",
    "            total_penalties += penalties\n",
    "            total_epochs += epochs\n",
    "\n",
    "        print(\"Results after {episodes} episodes:\")\n",
    "        print(\"Average timesteps per episode:\", total_epochs/episodes)\n",
    "        print(\"Average penalties per episode:\", total_penalties/episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arg:\n",
    "    def __init__(self):\n",
    "        self.seed = 1234\n",
    "        self.tau = 0.001\n",
    "        self.learning_rate =0.001\n",
    "        self.batch_size=64\n",
    "        self.bufferlength=2000\n",
    "        self.l2_decay=0.01\n",
    "        self.gamma=0.6\n",
    "        self.episode_len=1000\n",
    "        self.episode_steps=1000\n",
    "        self.epsilon=0.01\n",
    "        self.epsilon_decay=0.999\n",
    "        self.is_train=True\n",
    "        self.actor_weights='ddqn_cartpole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from time import time\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as weight_init\n",
    "\n",
    "def main(args):\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    \n",
    "    env = gym.make(\"Taxi-v2\").env\n",
    "    state_dim = 1\n",
    "    action_dim = 6\n",
    "\n",
    "    qfunction = QFunction(state_dim, action_dim, learning_rate = args.learning_rate, epsilon = args.epsilon, seed = args.seed, batch_size = args.batch_size, tau = args.tau)\n",
    "\n",
    "    if CUDA: \n",
    "        qfunction = qfunction.cuda()\n",
    "\n",
    "    replay_buffer = ReplayBuffer(args.bufferlength)\n",
    "\n",
    "    agent = DDQNAgent(qfunction, replay_buffer, episode_len = args.episode_len,\n",
    "                     episode_steps=args.episode_steps, epsilon = args.epsilon, epsilon_decay = args.epsilon_decay,\n",
    "                     batch_size = args.batch_size, gamma = args.gamma, seed = args.seed)\n",
    "\n",
    "    if args.is_train:\n",
    "        agent.train(env)\n",
    "        agent.test(env)\n",
    "        #agent.save_critic_weights(save_dir=OUTPUT_RESULTS_DIR, filename=args.actor_weights)       \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = arg()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
